{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest 10-K URL: https://www.sec.gov/Archives/edgar/data/0000875320/000087532025000053/vrtx-20241231.htm\n",
      "Filing Date: 2025-02-13\n",
      "\n",
      "\n",
      "Full 10-K saved as 'VRTX_10k.txt'.\n",
      "Extracted product-related section (39 lines).\n",
      "Error parsing JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response was: <think>\n",
      "Alright, so I need to extract product announcements from VRTX's SEC 10-K filing for the date January 25, 2025. The user provided a detailed excerpt with various news items related to their new products and developments.\n",
      "\n",
      "First, I should understand what exactly is being requested. The task is to create an array of JSON objects with specific keys: company_name, stock_name, filing_time, new_product, and product_description. If there are no announcements found, return an empty array. Also, only valid JSON without any commentary should be included.\n",
      "\n",
      "Looking at the provided text, I see several announcements. The FDA approved JOURNAVX for moderate-to-severe acute pain in January 2025. Then, they're enrolling in a Phase 3 trial for diabetic peripheral neuropathy and another for Lumbosacral Radiculopathy. These all seem to be new product developments.\n",
      "\n",
      "Next, in December 2024, the FDA announced results from a Phase 2 trial evaluating suzetrigine for LSR pain. This adds another significant announcement. Additionally, their next-generation pain inhibitor VX-993 was also under review in two phases.\n",
      "\n",
      "They mention a serial innovation approach and focus on increasing the likelihood of bringing transformative medicines to patients, aiming for durability in clinical success. Their marketing strategy includes expanding access to CF patients through approvals of new medicines, geographies, labels, and expanded reimbursement. They're also diversifying their business with CASGEVY treatment for SCD and TDT.\n",
      "\n",
      "Another announcement about JOURNAVX's global launch into pivotal development during the last year, including patient engagement, patient journey support, payors, ATC involvement, manufacturing, distribution, education, and financial assistance programs.\n",
      "\n",
      "I need to extract each of these announcements, ensuring I only include them if they meet the criteria. Each entry should be a JSON object with all required keys. If an announcement doesn't fit (like unclear details), it might not qualify as per their instructions.\n",
      "\n",
      "Let me start parsing each paragraph. The FDA approval for JOURNAVX in January seems clear and important, so that's one entry. Their enrolling in LSR trial is another point to include. Then the Phase 2 results from December on suzetrigine—another key announcement. VX-993’s status is another item.\n",
      "\n",
      "They also mention their innovation strategy, product diversification, global access, CLMs, manufacturing/Supply chain expansion, education efforts, and financial support programs. Each of these is a separate point to include.\n",
      "\n",
      "So I'll go through each sentence or paragraph in the text, identify the main announcements, and extract them into JSON objects. I have to make sure that each object has all five fields accurately represented without missing anything crucial.\n",
      "</think>\n",
      "\n",
      "{\n",
      "  \"company_name\": \"VRTX\",\n",
      "  \"stock_name\": \"JOURNAVX\",\n",
      "  \"filing_time\": \"2025-02-13\",\n",
      "  \"new_product\": \"JOURNAVX (suzetrigine)\",\n",
      "  \"product_description\": \" selective non-opioid NaV1.8 pain signal inhibitor approved for moderate-to-severe acute pain and Phase 3 trials evaluating suzetrigine in diabetic peripheral neuropathy.\"\n",
      "}\n",
      "\n",
      "{\n",
      "  \"company_name\": \"VRTX\",\n",
      "  \"stock_name\": \"JOURNAVX\",\n",
      "  \"filing_time\": \"2025-02-13\",\n",
      "  \"new_product\": \"JOURNAVX (suzetrigine)\",\n",
      "  \"product_description\": \" Phase 2 clinical trial results for suzetrigine in people with LSR.\"\n",
      "}\n",
      "\n",
      "{\n",
      "  \"company_name\": \"VRTX\",\n",
      "  \"stock_name\": \"SULZIGINE\",\n",
      "  \"filing_time\": \"2025-02-13\",\n",
      "  \"new_product\": \"sbezirine\",\n",
      "  \"product_description\": \" Next-generation selective NaV1.8 pain signal inhibitor under review for acute pain and diabetic peripheral neuropathy.\"\n",
      "}\n",
      "\n",
      "{\n",
      "  \"company_name\": \"VRTX\",\n",
      "  \"stock_name\": \"ALYFTREK\",\n",
      "  \"filing_time\": \"2025-02-13\",\n",
      "  \"new_product\": \"ALYFTREK (triptansin)\",\n",
      "  \"product_description\": \" Triple combination regimen with once-daily dosing, lower royalty burden, leading to a new standard in CF care.\"\n",
      "}\n",
      "\n",
      "{\n",
      "  \"company_name\": \"VRTX\",\n",
      "  \"stock_name\": \"JOURNAVX\",\n",
      "  \"filing_time\": \"2025-02-13\",\n",
      "  \"new_product\": \"JOURNAVX (sbezirine)\",\n",
      "  \"product_description\": \" Global pivotal development for JOURNAVX.\"\n",
      "}\n",
      "\n",
      "Extracted Products: []\n",
      "No product details extracted.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "########################\n",
    "# Mappings\n",
    "########################\n",
    "CIK_TO_COMPANY = {\n",
    "    \"0000789019\": \"Microsoft\", \n",
    "    \"320193\": \"Apple\", \n",
    "    \"1045810\": \"NVIDIA\", \n",
    "    \"1018724\": \"Amazon\",\n",
    "    \"0001652044\": \"Alphabet\", \n",
    "    \"40545\": \"General Electric\",\n",
    "    \"50863\": \"Intel\", \n",
    "    \"0001318605\": \"Tesla\",\n",
    "    \"1326801\" : \"Meta Platforms\",\n",
    "    \"1067983\": \"Berkshire Hathaway\",\n",
    "    \"104169\": \"Walmart\", \n",
    "    \"1403161\": \"Visa\",\n",
    "    \"19617\": \"JPMorgan Chase\", \n",
    "    \"1141391\": \"Mastercard\", \n",
    "    \"34088\": \"Exxon Mobil\", \n",
    "    \"731766\": \"UnitedHealth Group\",\n",
    "    \"909832\": \"Costco\", \n",
    "    \"80424\": \"Proctor & Gamble\",\n",
    "    \"1341439\": \"Oracle\", \n",
    "    \"200406\": \"Johnson & Johnson\",\n",
    "    \"1065280\": \"Netflix\", \n",
    "    \"354950\": \"Home Depot\", \n",
    "    \"21344\": \"Coke\", \n",
    "    \"70858\": \"Bank of America\",\n",
    "    \"93410\": \"Chevron\",\n",
    "    \"858877\": \"Cisco\", \n",
    "    \"72971\": \"Well Fargo\",\n",
    "    \"77476\": \"Pepsi\", \n",
    "    \"1321655\": \"Palantir\", \n",
    "    \"63908\": \"McDonald's\",\n",
    "    \"732717\": \"AT&T\", \n",
    "    \"732712\": \"Verizon\",\n",
    "    \"895421\": \"Morgan Stanley\", \n",
    "    \"1744489\": \"Disney\", \n",
    "    \"318154\": \"Amgen\", \n",
    "    \"97476\": \"Texas Instruments\",\n",
    "    \"18230\": \"Caterpillar\", \n",
    "    \"2488\": \"AMD\", \n",
    "    \"78003\": \"Pfizer\", \n",
    "    \"1283699\": \"T-Mobile\",\n",
    "    \"101829\": \"RTX\", \n",
    "    \"885725\": \"Boston Scientific\", \n",
    "    \"1166691\": \"Comcast\", \n",
    "    \"12927\": \"Boeing\",\n",
    "    \"936468\": \"Lockheed Martin\", \n",
    "    \"320187\": \"Nike\", \n",
    "    \"886982\": \"Goldman Sachs\", \n",
    "    \"1633917\": \"Paypal\",\n",
    "    \"1571996\": \"Dell\", \n",
    "    \"37996\": \"Ford\", \n",
    "    \"33185\": \"Equifax\", \n",
    "    \"73309\": \"Nucor\",\n",
    "    \"14272\": \"Bristol\", \n",
    "    \"1730168\": \"Broadcom\", \n",
    "    \"804328\": \"Qualcomm\",\n",
    "    \"1551152\": \"Abbvie\", \n",
    "    \"1413329\": \"Philip Morris\",\n",
    "    \"310158\": \"Merck Co.\", \n",
    "    \"1326160\": \"Duke Energy\",\n",
    "    \"1739940\": \"Cigna\", \n",
    "    \"1800\": \"Abbott Laboratories\",\n",
    "    \"66740\": \"3M\", \n",
    "    \"21665\": \"Colagate-Palmolive\",\n",
    "    \"796343\": \"Adobe\", \n",
    "    \"875320\": \"Vertex Pharmaceuticals\",\n",
    "}\n",
    "\n",
    "TICKER_TO_CIK = {\n",
    "    \"MSFT\": \"0000789019\", \"AAPL\": \"0000320193\", \"NVDA\": \"1045810\", \"AMZN\": \"0001018724\",\n",
    "    \"GOOGL\": \"0001652044\", \"GE\": \"0000040545\", \"INTC\": \"50863\", \"TSLA\": \"0001318605\",\n",
    "    \"META\": \"1326801\", \"BRK\": \"0001067983\", \"WMT\": \"0000104169\", \"V\": \"0001403161\",\n",
    "    \"JPM\": \"0000019617\", \"MA\": \"0001141391\", \"XOM\": \"0000034088\", \"UNH\": \"0000731766\",\n",
    "    \"COST\": \"0000909832\", \"PG\": \"0000080424\", \"ORCL\": \"0001341439\", \"JNJ\": \"0000200406\",\n",
    "    \"NFLX\": \"0001065280\", \"HD\": \"0000354950\", \"KO\": \"0000021344\", \"BAC\": \"0000070858\",\n",
    "    \"CVX\": \"0000093410\", \"CSCO\": \"0000858877\", \"WFC\": \"0000072971\", \"PEP\": \"0000077476\",\n",
    "    \"PLTR\": \"0001321655\", \"MCD\": \"0000063908\", \"T\": \"0000732717\", \"VZ\": \"0000732712\",\n",
    "    \"MS\": \"0000895421\", \"DIS\": \"0001744489\", \"AMGN\": \"0000318154\", \"TXN\": \"0000097476\",\n",
    "    \"CAT\": \"0000018230\", \"AMD\": \"0000002488\", \"PFE\": \"0000078003\", \"TMUS\": \"0001283699\",\n",
    "    \"RTX\": \"000101829\", \"BSX\": \"0000885725\", \"CMCSA\": \"0001166691\", \"BA\": \"0000012927\",\n",
    "    \"LMT\": \"0000936468\", \"NKE\": \"0000320187\", \"GS\": \"0000886982\", \"PYPL\": \"0001633917\",\n",
    "    \"DELL\": \"0001571996\", \"F\": \"0000037996\", \"EFX\": \"0000033185\", \"NUE\": \"0000073309\",\n",
    "    \"BMY\": \"0000014272\", \"AVGO\": \"0001730168\", \"QCOM\": \"0000804328\",\n",
    "    \"ABBV\": \"0001551152\", \"PM\": \"0001413329\",\n",
    "    \"MRK\": \"0000310158\", \"DUK\": \"0001326160\", \"CI\": \"0001739940\",\n",
    "    \"ABT\": \"0001800\", \"MMM\": \"0000066740\", \"CL\": \"0000021665\",\n",
    "    \"ADBE\": \"0000796343\", \"VRTX\": \"0000875320\",\n",
    "}\n",
    "\n",
    "########################\n",
    "# 1) Fetch + Parse 10-K\n",
    "########################\n",
    "def fetch_latest_10k(cik):\n",
    "    cik_padded = cik.zfill(10)\n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik_padded}.json\"\n",
    "    headers = {\"User-Agent\": \"Your Name (your@email.com)\"}\n",
    "    \n",
    "    r = requests.get(url, headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        print(\"Error fetching SEC data:\", r.status_code)\n",
    "        return None, None\n",
    "    \n",
    "    data = r.json()\n",
    "    filings = data[\"filings\"][\"recent\"]\n",
    "    \n",
    "    for i, form_type in enumerate(filings[\"form\"]):\n",
    "        if form_type == \"10-K\":\n",
    "            accession_num = filings[\"accessionNumber\"][i].replace(\"-\", \"\")\n",
    "            primary_doc = filings[\"primaryDocument\"][i]\n",
    "            filing_date = filings[\"filingDate\"][i]\n",
    "            ten_k_url = f\"https://www.sec.gov/Archives/edgar/data/{cik_padded}/{accession_num}/{primary_doc}\"\n",
    "            print(f\"Latest 10-K URL: {ten_k_url}\")\n",
    "            print(f\"Filing Date: {filing_date}\\n\")\n",
    "            return ten_k_url, filing_date\n",
    "    \n",
    "    print(\"No recent 10-K found.\")\n",
    "    return None, None\n",
    "\n",
    "def extract_10k_text(url, ticker):\n",
    "    headers = {\"User-Agent\": \"Your Name (your@email.com)\"}\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        print(\"Error fetching 10-K:\", resp.status_code)\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    text_10k = soup.get_text(separator=\"\\n\")\n",
    "    \n",
    "    filename = f\"{ticker}_10k.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text_10k)\n",
    "    \n",
    "    print(f\"\\nFull 10-K saved as '{filename}'.\")\n",
    "    return text_10k\n",
    "\n",
    "###############################\n",
    "# 2) Heuristic Extract (Optional)\n",
    "###############################\n",
    "def extract_product_section(text):\n",
    "    \"\"\"\n",
    "    Using a simple keyword approach. If none found, fallback to entire text.\n",
    "    \"\"\"\n",
    "    keywords = ['new product', 'launch', 'introduced', 'announced', 'released']\n",
    "    lines = text.splitlines()\n",
    "    relevant = [line for line in lines if any(kw in line.lower() for kw in keywords)]\n",
    "    if relevant:\n",
    "        print(f\"Extracted product-related section ({len(relevant)} lines).\")\n",
    "        return \"\\n\".join(relevant)\n",
    "    else:\n",
    "        print(\"No product-related lines found; using full text.\")\n",
    "        return text\n",
    "\n",
    "###############################\n",
    "# 3) JSON Fallback\n",
    "###############################\n",
    "def extract_json_from_response(raw_response):\n",
    "    match = re.search(r'(\\[.*\\])', raw_response, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            print(\"Regex extraction error:\", e)\n",
    "    return []\n",
    "\n",
    "###############################\n",
    "# 4) LLM Query\n",
    "###############################\n",
    "def query_ollama_json(text, company_name, stock_name, filing_time):\n",
    "    ollama_url = \"http://localhost:11434/api/generate\"\n",
    "    prompt = (\n",
    "        f\"Below is a SEC 10-K excerpt for {company_name} (Ticker: {stock_name}, Filing: {filing_time}).\\n\"\n",
    "        \"Extract new product announcements (incl expansions or redesigns). Return an array of JSON objects with keys:\\n\"\n",
    "        '  \"company_name\", \"stock_name\", \"filing_time\", \"new_product\", \"product_description\"\\n'\n",
    "        \"If none found, return an empty JSON array []. Valid JSON only, no commentary.\\n\\n\"\n",
    "        f\"Text:\\n{text[:5000]}\"\n",
    "    )\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"deepseek-r1:1.5b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        r = requests.post(ollama_url, json=payload)\n",
    "        if r.status_code == 200:\n",
    "            resp = r.json()\n",
    "            raw_output = resp.get(\"response\", \"\")\n",
    "            # Attempt direct parse\n",
    "            try:\n",
    "                out = json.loads(raw_output)\n",
    "                return out\n",
    "            except Exception as e:\n",
    "                print(\"Error parsing JSON:\", e)\n",
    "                print(\"Raw response was:\", raw_output)\n",
    "                # Fallback\n",
    "                return extract_json_from_response(raw_output)\n",
    "        else:\n",
    "            print(f\"Error {r.status_code}: {r.text}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(\"Error querying LLM:\", e)\n",
    "        return []\n",
    "\n",
    "###############################\n",
    "# 5) CSV Writer (Filter Non-Dict)\n",
    "###############################\n",
    "def write_products_to_csv(company_name, stock_name, filing_time, products, csv_filename=\"extracted_products.csv\"):\n",
    "    \"\"\"\n",
    "    We'll filter out any items that aren't dicts to avoid 'str' object has no attribute 'get'.\n",
    "    \"\"\"\n",
    "    write_header = not os.path.exists(csv_filename)\n",
    "    with open(csv_filename, \"a\", newline=\"\", encoding=\"utf-8\") as cf:\n",
    "        writer = csv.writer(cf)\n",
    "        if write_header:\n",
    "            writer.writerow([\"Company Name\", \"Stock Name\", \"Filing Time\", \"New Product\", \"Product Description\"])\n",
    "        \n",
    "        # Filter non-dict entries\n",
    "        valid_dicts = [p for p in products if isinstance(p, dict)]\n",
    "        \n",
    "        for p in valid_dicts:\n",
    "            writer.writerow([\n",
    "                p.get(\"company_name\", company_name),\n",
    "                p.get(\"stock_name\", stock_name),\n",
    "                p.get(\"filing_time\", filing_time),\n",
    "                p.get(\"new_product\", \"\"),\n",
    "                p.get(\"product_description\", \"\")\n",
    "            ])\n",
    "    print(f\"\\nProducts appended to '{csv_filename}' successfully!\")\n",
    "\n",
    "###############################\n",
    "# 6) Main Pipeline\n",
    "###############################\n",
    "def test_pipeline():\n",
    "    ticker = input(\"Enter Ticker (e.g. MSFT, AAPL): \").strip().upper()\n",
    "    \n",
    "    cik = TICKER_TO_CIK.get(ticker)\n",
    "    if not cik:\n",
    "        print(f\"Ticker {ticker} not found in TICKER_TO_CIK map.\")\n",
    "        return\n",
    "    company_name = CIK_TO_COMPANY.get(cik, ticker)\n",
    "    \n",
    "    ten_k_url, filing_date = fetch_latest_10k(cik)\n",
    "    if not ten_k_url:\n",
    "        return\n",
    "    \n",
    "    filing_text = extract_10k_text(ten_k_url, ticker)\n",
    "    if not filing_text or len(filing_text) < 100:\n",
    "        print(\"Failed to extract enough text.\")\n",
    "        return\n",
    "    \n",
    "    # Simple product section approach\n",
    "    product_section = extract_product_section(filing_text)\n",
    "    \n",
    "    # LLM Query\n",
    "    extracted_products = query_ollama_json(product_section, company_name, ticker, filing_date)\n",
    "    print(\"\\nExtracted Products:\", extracted_products)\n",
    "    \n",
    "    # CSV Write (filter out strings)\n",
    "    if extracted_products:\n",
    "        write_products_to_csv(company_name, ticker, filing_date, extracted_products)\n",
    "    else:\n",
    "        print(\"No product details extracted.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
